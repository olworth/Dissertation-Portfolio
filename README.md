# Dissertation Portfolio

This repo serves as a portfolio for the code I wrote for my dissertation, _Exploring and Understanding Cross-Lingual Transfer to Catalan and Galician via High-Resource Typological Relatives_. 

The core of the project was an investigation into NLP tasks in Catalan and Galician, two low-resource languages that lack the vast amounts of textual data necessary to pre-train and fine-tune good LLMs. By leveraging textual data from higher-resource languages in a transfer learning paradigm, however, (for example, pre-training a model in Spanish and fine-tuning it in Catalan) performance can be improved. My results indicated that this improvement was generally greatest when data from multiple languages were used in pre-training; the best combination of languages to use, however, depended on the nature of the NLP task at hand and of each language's relationship with the target language. 

A particularly pertinent example is that of my Dependency Parsing models: whereas Galician models preferred a small number of closely-related languages, their Catalan equivalents benefitted substantially more from a massively multilingual pre-training, spanning over a hundred languages. I theorised that this was the result of Galician morphology and syntax sitting in more of a "sweet spot" of similarity in relation to its close typological relatives, and attempted to quantify this.

If you would like to play around with some of the models I created, click [here](https://huggingface.co/homersimpson) for my HuggingFace account. HuggingFace's Inference API should facilitate some limited interaction with them, but if not they can be freely downloaded. If you would like to read my dissertation in its entirety, please contact me and I will provide access!

The example scripts contained in this repository are as follows:

* [belebele-train.py](https://github.com/olworth/Dissertation-Portfolio/blob/main/example-scripts/belebele-natural-language-understanding/belebele-train.py), [belebele-eval.py](https://github.com/olworth/Dissertation-Portfolio/blob/main/example-scripts/belebele-natural-language-understanding/belebele-eval.py)
    * Code to fine-tune and evaluate a model for the [Belebele Natural Language Understanding (NLU) benchmark](https://github.com/facebookresearch/belebele), using HuggingFace. I used the encoder-only RoBERTa transformer architecture for this task. This is a mutliple-choice reading comprehension task, and as such necessitated Catalan and Galician multiple-choice QA datasets for training - neither of which existed. I therefore had to create my own: first assembling an English training set using the recommended [assemble_training_set.py](https://github.com/facebookresearch/belebele/blob/main/assemble_training_set.py), and then mass translating this into the desired target languages.
* [translate-belebele-dataset.py](https://github.com/olworth/Dissertation-Portfolio/blob/main/example-scripts/belebele-translate-dataset/translate-belebele-dataset.py)
    * Code I used to translate the large (71.36K rows, each more or less a whole sentence) Belebele training sets into Catalan and Galician en masse. I had to prioritise speed and efficiency due to the limited computational resources at my disposal, but found the resulting translations were still of good quality. 
* [dependency-parsing](https://github.com/olworth/Dissertation-Portfolio/tree/main/example-scripts/dependency-parsing)
    * Code to fine-tune and evaluate a transformer with a deep biaffine parser head for dependency parsing, using the [SuPar](https://github.com/yzhangcs/parser) library. I also included some of the logs produced. 
* [ner-train.py](https://github.com/olworth/Dissertation-Portfolio/blob/main/example-scripts/named-entity-recognition/ner-train.py), [ner-eval.py](https://github.com/olworth/Dissertation-Portfolio/blob/main/example-scripts/named-entity-recognition/ner-eval.py)
    * Code to fine-tune and evaluate a model for Named Entity Recognition (NER), using HuggingFace. I used the encoder-only RoBERTa transformer architecture for this task.
* [pos-train.py](https://github.com/olworth/Dissertation-Portfolio/blob/main/example-scripts/part-of-speech-tagging/pos-train.py), [pos-eval.py](https://github.com/olworth/Dissertation-Portfolio/blob/main/example-scripts/part-of-speech-tagging/pos-eval.py)
    * Code to fine-tune and evaluate a model for Part-Of-Speech (POS) Tagging, using HuggingFace. I used the encoder-only RoBERTa transformer architecture for this task.
* [summarisation-train.py](https://github.com/olworth/Dissertation-Portfolio/blob/main/example-scripts/summarisation/summarisation-train.py), [summarisation-eval.py](https://github.com/olworth/Dissertation-Portfolio/blob/main/example-scripts/summarisation/summarisation-eval.py)
    * Code to fine-tune and evaluate a model for Automatic Text Summarisation, using HuggingFace. I used the encoder-decoder T5 transformer architecture for this task. 
* [translation-train.py](https://github.com/olworth/Dissertation-Portfolio/blob/main/example-scripts/translation/translation-train.py), [translation-eval.py](https://github.com/olworth/Dissertation-Portfolio/blob/main/example-scripts/translation/translation-eval.py), [translation-eval-chrf.py](https://github.com/olworth/Dissertation-Portfolio/blob/main/example-scripts/translation/translation-eval-chrf.py)
    * Code to fine-tune and evaluate a model for Machine Translation, using HuggingFace. I used the encoder-decoder T5 transformer architecture for this task. The two evaluation files are for the two separate evaluation metrics I employed.
* [trim-mt5.py](https://github.com/olworth/Dissertation-Portfolio/blob/main/example-scripts/trim-mt5/trim-mt5.py)
    * Code to trim the vocabulary of the T5-architecture Massively Multilingual Transformer (MMT), mT5-base. Due to resource limitations, I was not able to perform any model pre-training myself, and as such was highly limited in the scope of what multilingual models I was able to use. To alleviate this problem, I simulated my desired pre-training by performing a "trimming" process on an MMT, a multilingual model trained on over 100 languages. This involved processing a large corpus in each of my desired source languages, selecting the most frequent 30,000 tokens in each, and updating the model's tokenizer voabulary and input and output embeddings to include only these.

        The code for this trimming process was adapted from [David Dale's](https://colab.research.google.com/gist/avidale/44cd35bfcdaf8bedf51d97c468cc8001/create_rut5-base.ipynb), in turn inspired by [Abdaoui et al. (2020)](https://arxiv.org/abs/2010.05609). It should be noted that the process does not produce an entirely accurate replication of a model pre-trained solely on the desired languages - the embeddings in the trimmed model retain the same weights as those in the MMT, trained on over a 100 languages. However, it was a faithul enough simulation for my purposes. 

* [trim-xlm-r.py](https://github.com/olworth/Dissertation-Portfolio/blob/main/example-scripts/trim-xlm-r/trim-xlm-r.py)
    * Code to trim the vocabulary of the RoBERTa-architecture MMT XLM-R-base, in the same manner as described above. An adaptation of [David Dale's code](https://colab.research.google.com/drive/1f-n3zBQjmtMrp7oHzvunHPSC5aIMNe_N?usp=sharing).
